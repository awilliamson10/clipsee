{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
    "\n",
    "@dataclass\n",
    "class SAETrainConfig(LanguageModelSAERunnerConfig):\n",
    "    dataset_path: str = 'imagenet_data'\n",
    "    num_workers: int = 0\n",
    "    num_epochs: int = 3\n",
    "\n",
    "    expansion_factor: int = 24\n",
    "    context_size: int = 257\n",
    "    d_in: int = 1024\n",
    "    model_name: str = \"laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K\"\n",
    "    hook_point: str = \"blocks.{layer}.hook_mlp_out\"\n",
    "    hook_point_layer: List[int] = field(default_factory=lambda: [22])\n",
    "    dead_feature_window: int = 2500\n",
    "    use_ghost_grads: bool = True\n",
    "    feature_sampling_window: int = 250\n",
    "    from_pretrained_path: str = None\n",
    "\n",
    "    b_dec_init_method: str = \"geometric_median\"\n",
    "    normalize_sae_decoder: bool = True\n",
    "\n",
    "    lr: float = 0.0005\n",
    "    l1_coefficient: int = 0.006\n",
    "    lr_scheduler_name: str = \"cosineannealing\"\n",
    "    train_batch_size_tokens: int = 1\n",
    "    lr_warm_up_steps: int = 4000\n",
    "\n",
    "    n_batches_in_buffer: int = 2\n",
    "    store_batch_size: int = 1\n",
    "\n",
    "    log_to_wandb: bool = True\n",
    "    wandb_project: str = \"openclip_sae_training\"\n",
    "    wandb_entity: str = \"willfulbytes\"\n",
    "    wandb_log_frequency: int = 25\n",
    "    eval_every_n_wandb_logs: int = 10\n",
    "    run_name: str = None\n",
    "\n",
    "    device: str = \"mps\"\n",
    "    seed: int = 42\n",
    "    n_checkpoints: int = 10\n",
    "    checkpoint_path: str = \"checkpoints24\"\n",
    "    dtype: str = \"torch.float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1117d5c10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from vit_prisma.models.base_vit import HookedViT\n",
    "from open_clip import tokenize\n",
    "import datasets\n",
    "from typing import Any, Iterator, cast\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class HFDataset(Dataset):\n",
    "    def __init__(self, data_location, transforms, image_col, text_col):\n",
    "        self.dataset = datasets.load_dataset(data_location, split=\"train\")\n",
    "        self.image_col = image_col\n",
    "        self.text_col = text_col\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Remove the extra dimension by squeezing the tensor\n",
    "        images = self.transforms(self.dataset[idx][self.image_col], return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "        texts = tokenize([self.dataset[idx][self.text_col]])[0]\n",
    "        return images, texts\n",
    "\n",
    "# Update the collate functions accordingly\n",
    "def collate_fn(data):\n",
    "    imgs, _ = zip(*data)\n",
    "    return torch.stack(imgs, dim=0)\n",
    "\n",
    "def collate_fn_eval(data):\n",
    "    imgs, texts = zip(*data)\n",
    "    return torch.stack(imgs, dim=0), torch.stack(texts, dim=0)\n",
    "\n",
    "\n",
    "class OpenCLIPActivationsStore:\n",
    "    \"\"\"\n",
    "    Class for streaming tokens and generating and storing activations\n",
    "    while training SAEs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: SAETrainConfig,\n",
    "        model: HookedViT,\n",
    "        dataset: torch.utils.data.Dataset,\n",
    "        eval_dataset: torch.utils.data.Dataset = None,\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        self.config = config\n",
    "        assert (\n",
    "            not self.config.normalize_activations\n",
    "        ), \"Normalize activations is currently not implemented for vision, sorry!\"\n",
    "        self.normalize_activations = self.config.normalize_activations\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "\n",
    "        self.image_dataloader = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.config.store_batch_size,\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        self.image_dataloader_eval = torch.utils.data.DataLoader(\n",
    "            self.eval_dataset,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.config.store_batch_size,\n",
    "            collate_fn=collate_fn_eval,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        self.image_dataloader_iter = self.get_batch_tokens_internal()\n",
    "        self.image_dataloader_eval_iter = self.get_val_batch_tokens_internal()\n",
    "\n",
    "        self.storage_buffer = self.get_buffer(self.config.n_batches_in_buffer // 2)\n",
    "        self.dataloader = self.get_data_loader()\n",
    "\n",
    "\n",
    "    def get_batch_tokens_internal(self):\n",
    "        \"\"\"\n",
    "        Streams a batch of tokens from a dataset.\n",
    "        \"\"\"\n",
    "        device = self.config.device\n",
    "        while True:\n",
    "            for data in self.image_dataloader:\n",
    "                data.requires_grad_(False)\n",
    "                yield data.to(device)  # 5\n",
    "\n",
    "    def get_batch_tokens(self):\n",
    "        return next(self.image_dataloader_iter)\n",
    "\n",
    "    # returns the ground truth class as well.\n",
    "    def get_val_batch_tokens_internal(self):\n",
    "        \"\"\"\n",
    "        Streams a batch of tokens from a dataset.\n",
    "        \"\"\"\n",
    "        device = self.config.device\n",
    "        while True:\n",
    "            for image_data, labels in self.image_dataloader_eval:\n",
    "                image_data.requires_grad_(False)\n",
    "                labels.requires_grad_(False)\n",
    "                yield image_data.to(device), labels.to(device)\n",
    "\n",
    "    def get_val_batch_tokens(self):\n",
    "        return next(self.image_dataloader_eval_iter)\n",
    "\n",
    "    def get_activations(self, batch_tokens: torch.Tensor, get_loss: bool = False):\n",
    "        \"\"\"\n",
    "        Returns activations of shape (batches, context, num_layers, d_in)\n",
    "        \"\"\"\n",
    "        layers = (\n",
    "            self.config.hook_point_layer\n",
    "            if isinstance(self.config.hook_point_layer, list)\n",
    "            else [self.config.hook_point_layer]\n",
    "        )\n",
    "        act_names = [self.config.hook_point.format(layer=layer) for layer in layers]\n",
    "        hook_point_max_layer = max(layers)\n",
    "\n",
    "        if self.config.hook_point_head_index is not None:\n",
    "            layerwise_activations = self.model.run_with_cache(\n",
    "                batch_tokens,\n",
    "                names_filter=act_names,\n",
    "                stop_at_layer=hook_point_max_layer + 1,\n",
    "            )[1]\n",
    "            activations_list = [\n",
    "                layerwise_activations[act_name][:, :, self.config.hook_point_head_index]\n",
    "                for act_name in act_names\n",
    "            ]\n",
    "        else:\n",
    "            layerwise_activations = self.model.run_with_cache(  ####\n",
    "                batch_tokens,\n",
    "                names_filter=act_names,\n",
    "                stop_at_layer=hook_point_max_layer + 1,\n",
    "            )[1]\n",
    "            activations_list = [\n",
    "                layerwise_activations[act_name] for act_name in act_names\n",
    "            ]\n",
    "\n",
    "        # Stack along a new dimension to keep separate layers distinct\n",
    "        stacked_activations = torch.stack(activations_list, dim=2)\n",
    "\n",
    "        return stacked_activations\n",
    "\n",
    "    def get_buffer(self, n_batches_in_buffer: int):\n",
    "        context_size = self.config.context_size\n",
    "        batch_size = self.config.store_batch_size\n",
    "        d_in = self.config.d_in\n",
    "        total_size = batch_size * n_batches_in_buffer\n",
    "        num_layers = (\n",
    "            len(self.config.hook_point_layer)\n",
    "            if isinstance(self.config.hook_point_layer, list)\n",
    "            else 1\n",
    "        )  # Number of hook points or layers\n",
    "\n",
    "        refill_iterator = range(0, batch_size * n_batches_in_buffer, batch_size)\n",
    "        # Initialize empty tensor buffer of the maximum required size with an additional dimension for layers\n",
    "        new_buffer = torch.zeros(\n",
    "            (total_size, context_size, num_layers, d_in),\n",
    "            dtype=self.config.dtype,\n",
    "            device=self.config.device,\n",
    "        )\n",
    "\n",
    "        for refill_batch_idx_start in refill_iterator:\n",
    "            refill_batch_tokens = self.get_batch_tokens()  ######\n",
    "            refill_activations = self.get_activations(refill_batch_tokens)\n",
    "\n",
    "            new_buffer[\n",
    "                refill_batch_idx_start : refill_batch_idx_start + batch_size, ...\n",
    "            ] = refill_activations\n",
    "\n",
    "            # pbar.update(1)\n",
    "\n",
    "        new_buffer = new_buffer.reshape(-1, num_layers, d_in)\n",
    "        new_buffer = new_buffer[torch.randperm(new_buffer.shape[0])]\n",
    "\n",
    "        return new_buffer\n",
    "\n",
    "    def get_data_loader(\n",
    "        self,\n",
    "    ) -> Iterator[Any]:\n",
    "        \"\"\"\n",
    "        Return a torch.utils.dataloader which you can get batches from.\n",
    "\n",
    "        Should automatically refill the buffer when it gets to n % full.\n",
    "        (better mixing if you refill and shuffle regularly).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = self.config.train_batch_size_tokens\n",
    "\n",
    "        # 1. # create new buffer by mixing stored and new buffer\n",
    "        mixing_buffer = torch.cat(\n",
    "            [self.get_buffer(self.config.n_batches_in_buffer // 2), self.storage_buffer], ####\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        mixing_buffer = mixing_buffer[torch.randperm(mixing_buffer.shape[0])]\n",
    "\n",
    "        # 2.  put 50 % in storage\n",
    "        self.storage_buffer = mixing_buffer[: mixing_buffer.shape[0] // 2]\n",
    "\n",
    "        # 3. put other 50 % in a dataloader\n",
    "        dataloader = iter(\n",
    "            DataLoader(\n",
    "                # TODO: seems like a typing bug?\n",
    "                cast(Any, mixing_buffer[mixing_buffer.shape[0] // 2 :]),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "        Get the next batch from the current DataLoader.\n",
    "        If the DataLoader is exhausted, refill the buffer and create a new DataLoader.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Try to get the next batch\n",
    "            return next(self.dataloader)\n",
    "        except StopIteration:\n",
    "            # If the DataLoader is exhausted, create a new one\n",
    "            self.dataloader = self.get_data_loader() #### 97\n",
    "            return next(self.dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 24576-L1-0.006-LR-0.0005-Tokens-2.000e+06\n",
      "n_tokens_per_buffer (millions): 0.016448\n",
      "Lower bound: n_contexts_per_buffer (millions): 6.4e-05\n",
      "Total training steps: 2000000\n",
      "Total wandb updates: 80000\n",
      "n_tokens_per_feature_sampling_window (millions): 0.06425\n",
      "n_tokens_per_dead_feature_window (millions): 0.6425\n",
      "We will reset the sparsity calculation 8000 times.\n",
      "Number tokens in sparsity calculation window: 2.50e+02\n",
      "Using Ghost Grads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aw/projects/vitsearch/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Ghost Grads.\n",
      "Using Ghost Grads.\n",
      "Using Ghost Grads.\n",
      "{'n_layers': 24, 'd_model': 1024, 'd_head': 64, 'model_name': '', 'n_heads': 16, 'd_mlp': 4096, 'activation_name': 'gelu', 'eps': 1e-05, 'original_architecture': 'vit_clip_vision_encoder', 'initializer_range': 0.02, 'n_channels': 3, 'patch_size': 14, 'image_size': 224, 'n_classes': 768, 'n_params': None, 'layer_norm_pre': True, 'return_type': 'class_logits'}\n",
      "LayerNorm folded.\n",
      "Centered weights writing to residual stream\n",
      "Loaded pretrained model laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K into HookedTransformer\n",
      "0: Name: laion_CLIP-ViT-L-14-DataComp.XL-s13B-b90K_blocks.22.hook_mlp_out_24576_hook_point_layer_22 Layer 22, p_norm 1, alpha 0.006\n"
     ]
    }
   ],
   "source": [
    "from sae_lens.training.sae_group import SparseAutoencoderDictionary\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "config = SAETrainConfig()\n",
    "processor = CLIPProcessor.from_pretrained(config.model_name)\n",
    "dataset = HFDataset(\"awilliamson/fashion-train\", processor.image_processor, \"image\", \"text\") # load_dataset(\"awilliamson/fashion-train\", split=\"train\")\n",
    "eval_dataset = HFDataset(\"awilliamson/fashion-eval\", processor.image_processor, \"image\", \"text\") # load_dataset(\"awilliamson/fashion-validation\", split=\"train\")\n",
    "# cfg.training_tokens = int(1_300_000*setup_args['num_epochs']) * cfg.context_size\n",
    "config.training_tokens = len(dataset) * config.num_epochs\n",
    "sae_group = SparseAutoencoderDictionary.load_from_pretrained(\"checkpoints24/final_40240/laion_CLIP-ViT-L-14-DataComp.XL-s13B-b90K_blocks.22.hook_mlp_out_24576_hook_point_layer_22\")\n",
    "model = HookedViT.from_pretrained(config.model_name, is_timm=False, is_clip=True)\n",
    "model.to(config.device)\n",
    "\n",
    "activation_store = OpenCLIPActivationsStore(\n",
    "    config = config,\n",
    "    model = model,\n",
    "    dataset = dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    num_workers = 0,\n",
    ")\n",
    "\n",
    "for i, (name, sae) in enumerate(sae_group):\n",
    "    hyp = sae.cfg\n",
    "    print(\n",
    "        f\"{i}: Name: {name} Layer {hyp.hook_point_layer}, p_norm {hyp.lp_norm}, alpha {hyp.l1_coefficient}\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen layer 22 hook point blocks.22.hook_mlp_out\n"
     ]
    }
   ],
   "source": [
    "AUTOENCODER_NAME = \"laion_CLIP-ViT-L-14-DataComp.XL-s13B-b90K_blocks.22.hook_mlp_out_24576_hook_point_layer_22\"\n",
    "sparse_autoencoder = sae_group.autoencoders[AUTOENCODER_NAME]\n",
    "sparse_autoencoder = sparse_autoencoder.to(config.device)\n",
    "layer_num = sparse_autoencoder.cfg.hook_point_layer\n",
    "print(f\"Chosen layer {layer_num} hook point {sparse_autoencoder.cfg.hook_point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average l0 802.49609375\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "bingroup": "x",
         "hovertemplate": "variable=0<br>value=%{x}<br>count=%{y}<extra></extra>",
         "legendgroup": "0",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "0",
         "offsetgroup": "0",
         "orientation": "v",
         "showlegend": true,
         "type": "histogram",
         "x": [
          774,
          864,
          697,
          690,
          839,
          731,
          810,
          878,
          863,
          819,
          856,
          1006,
          693,
          707,
          660,
          762,
          686,
          662,
          590,
          914,
          856,
          774,
          759,
          817,
          852,
          783,
          799,
          817,
          690,
          590,
          834,
          746,
          792,
          815,
          834,
          830,
          759,
          799,
          865,
          887,
          792,
          853,
          782,
          783,
          842,
          749,
          588,
          823,
          694,
          797,
          605,
          772,
          842,
          810,
          768,
          799,
          797,
          767,
          862,
          779,
          761,
          707,
          892,
          593,
          688,
          773,
          883,
          810,
          817,
          811,
          890,
          795,
          809,
          833,
          890,
          822,
          834,
          777,
          834,
          810,
          716,
          813,
          823,
          820,
          897,
          815,
          857,
          845,
          968,
          860,
          886,
          830,
          796,
          779,
          886,
          722,
          655,
          806,
          826,
          813,
          822,
          821,
          872,
          889,
          808,
          883,
          846,
          805,
          819,
          869,
          861,
          717,
          840,
          830,
          799,
          848,
          831,
          845,
          788,
          869,
          867,
          833,
          863,
          794,
          809,
          842,
          861,
          741,
          844,
          587,
          839,
          860,
          828,
          830,
          840,
          834,
          817,
          870,
          790,
          814,
          808,
          766,
          806,
          648,
          764,
          903,
          833,
          809,
          809,
          896,
          830,
          831,
          848,
          901,
          777,
          811,
          787,
          813,
          684,
          866,
          621,
          804,
          811,
          858,
          855,
          865,
          847,
          875,
          858,
          771,
          787,
          872,
          811,
          822,
          844,
          863,
          837,
          853,
          837,
          847,
          835,
          805,
          844,
          848,
          815,
          853,
          805,
          877,
          851,
          792,
          779,
          715,
          696,
          926,
          773,
          838,
          868,
          823,
          780,
          854,
          839,
          834,
          817,
          853,
          830,
          818,
          685,
          748,
          588,
          789,
          826,
          782,
          808,
          805,
          781,
          880,
          867,
          871,
          832,
          799,
          799,
          812,
          667,
          588,
          800,
          599,
          710,
          841,
          787,
          811,
          812,
          782,
          810,
          836,
          824,
          785,
          794,
          789,
          714,
          827,
          776,
          701,
          726,
          726,
          819,
          765,
          814,
          810,
          842,
          754,
          1032,
          838,
          733,
          700,
          713,
          776
         ],
         "xaxis": "x",
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "count"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "sparse_autoencoder.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "with torch.no_grad():\n",
    "    batch_tokens, labels = activation_store.get_val_batch_tokens()\n",
    "    _, cache = model.run_with_cache(batch_tokens)\n",
    "    sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(\n",
    "        cache[sparse_autoencoder.cfg.hook_point]\n",
    "    )\n",
    "    del cache\n",
    "\n",
    "    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
    "    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
    "    print(\"average l0\", l0.mean().item())\n",
    "    px.histogram(l0.flatten().cpu().numpy()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "\n",
    "@torch.no_grad()\n",
    "def highest_activating_tokens(\n",
    "    images,\n",
    "    model,\n",
    "    sparse_autoencoder,\n",
    "    W_enc,\n",
    "    b_enc,\n",
    "    k: int = 10,\n",
    "):\n",
    "    '''\n",
    "    Returns the indices & values for the highest-activating tokens in the given batch of data.\n",
    "    '''\n",
    "\n",
    "    # Get the post activations from the clean run\n",
    "    _, cache = model.run_with_cache(images)\n",
    "\n",
    "    inp = cache[sparse_autoencoder.cfg.hook_point]\n",
    "    b, seq_len, _ = inp.shape\n",
    "    post_reshaped = einops.rearrange( inp, \"batch seq d_mlp -> (batch seq) d_mlp\")\n",
    "    # Compute activations (not from a fwd pass, but explicitly, by taking only the feature we want)\n",
    "    # This code is copied from the first part of the 'forward' method of the AutoEncoder class\n",
    "    sae_in =  post_reshaped - sparse_autoencoder.b_dec # Remove decoder bias as per Anthropic\n",
    "\n",
    "    acts = einops.einsum(\n",
    "            sae_in,\n",
    "            W_enc,\n",
    "            \"... d_in, d_in n -> ... n\",\n",
    "        )\n",
    "    \n",
    "    acts = acts + b_enc\n",
    "    acts = torch.nn.functional.relu(acts)\n",
    "    #TODO clean up\n",
    "    unshape = einops.rearrange(acts, \"(batch seq) d_in -> batch seq d_in\", batch=b, seq=seq_len)\n",
    "    cls_acts = unshape[:,0,:]\n",
    "    per_image_acts = unshape.mean(1)\n",
    "\n",
    "\n",
    "\n",
    "    to_return = {} \n",
    "    # Get the top k activations for each image\n",
    "    top_k_indices = torch.topk(per_image_acts, k, dim=1).indices\n",
    "    top_k_values = torch.topk(per_image_acts, k, dim=1).values\n",
    "    to_return['top_k_indices'] = top_k_indices\n",
    "    to_return['top_k_values'] = top_k_values\n",
    "\n",
    "    # Get the top k activations for each class\n",
    "    top_k_cls_indices = torch.topk(cls_acts, k, dim=1).indices\n",
    "    top_k_cls_values = torch.topk(cls_acts, k, dim=1).values\n",
    "    to_return['top_k_cls_indices'] = top_k_cls_indices\n",
    "    to_return['top_k_cls_values'] = top_k_cls_values\n",
    "\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def analyze_feature_activations(\n",
    "    model,\n",
    "    sparse_autoencoder,\n",
    "    dataloader,\n",
    "    W_enc,\n",
    "    b_enc,\n",
    "    k=5,\n",
    "    device='cuda'\n",
    "):\n",
    "    model.eval()\n",
    "    sparse_autoencoder.eval()\n",
    "    feature_activations = {}\n",
    "    num_features = sparse_autoencoder.cfg.d_sae\n",
    "    \n",
    "    # Initialize feature_activations dictionary\n",
    "    for i in range(num_features):\n",
    "        feature_activations[i] = []\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Analyzing features\"):\n",
    "        images, texts = batch[0].to(device), batch[1]\n",
    "        \n",
    "        # Get the post activations from the clean run\n",
    "        _, cache = model.run_with_cache(images)\n",
    "        inp = cache[sparse_autoencoder.cfg.hook_point]\n",
    "        b, seq_len, _ = inp.shape\n",
    "        post_reshaped = einops.rearrange(inp, \"batch seq d_mlp -> (batch seq) d_mlp\")\n",
    "        \n",
    "        # Compute activations\n",
    "        sae_in = post_reshaped - sparse_autoencoder.b_dec\n",
    "        acts = einops.einsum(\n",
    "            sae_in,\n",
    "            W_enc,\n",
    "            \"... d_in, d_in n -> ... n\",\n",
    "        )\n",
    "        acts = acts + b_enc\n",
    "        acts = torch.nn.functional.relu(acts)\n",
    "        \n",
    "        unshape = einops.rearrange(acts, \"(batch seq) d_in -> batch seq d_in\", batch=b, seq=seq_len)\n",
    "        per_image_acts = unshape.mean(1)\n",
    "        \n",
    "        # Debug: Print shape and type of per_image_acts\n",
    "        # print(f\"Shape of per_image_acts: {per_image_acts.shape}\")\n",
    "        # print(f\"Type of per_image_acts: {type(per_image_acts)}\")\n",
    "        \n",
    "        # Update top k activations for each feature\n",
    "        for feature in range(num_features):\n",
    "            activations = per_image_acts[:, feature]\n",
    "            \n",
    "            # Debug: Print shape and type of activations\n",
    "            # print(f\"Shape of activations for feature {feature}: {activations.shape}\")\n",
    "            # print(f\"Type of activations for feature {feature}: {type(activations)}\")\n",
    "            \n",
    "            # Convert to numpy for easier handling\n",
    "            activations_np = activations.cpu().numpy()\n",
    "            top_k_indices = np.argsort(activations_np)[-k:][::-1]\n",
    "            \n",
    "            for idx in top_k_indices:\n",
    "                activation_value = activations_np[idx]\n",
    "                feature_activations[feature].append((activation_value, images[idx], texts[idx]))\n",
    "    \n",
    "    # Sort the results for each feature\n",
    "    for feature in feature_activations:\n",
    "        feature_activations[feature] = sorted(feature_activations[feature], key=lambda x: x[0], reverse=True)[:k]\n",
    "    \n",
    "    return feature_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing features:  23%|██▎       | 337/1491 [35:04<2:00:05,  6.24s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m top_activations \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_feature_activations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_autoencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_dataloader_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_autoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse_autoencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mb_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/vitsearch/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m, in \u001b[0;36manalyze_feature_activations\u001b[0;34m(model, sparse_autoencoder, dataloader, W_enc, b_enc, k, device)\u001b[0m\n\u001b[1;32m     27\u001b[0m images, texts \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Get the post activations from the clean run\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m _, cache \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m inp \u001b[38;5;241m=\u001b[39m cache[sparse_autoencoder\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mhook_point]\n\u001b[1;32m     32\u001b[0m b, seq_len, _ \u001b[38;5;241m=\u001b[39m inp\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/projects/vitsearch/.venv/lib/python3.11/site-packages/vit_prisma/models/base_vit.py:220\u001b[0m, in \u001b[0;36mHookedViT.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_cache\u001b[39m(\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mmodel_args, return_cache_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, remove_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    207\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m     Union[ActivationCache, Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]],\n\u001b[1;32m    213\u001b[0m ]:\n\u001b[1;32m    214\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper around `run_with_cache` in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m    If return_cache_object is True, this will return an ActivationCache object, with a bunch of\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;124;03m    useful HookedTransformer specific methods, otherwise it will return a dictionary of\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m     out, cache_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_batch_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremove_batch_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    224\u001b[0m         cache \u001b[38;5;241m=\u001b[39m ActivationCache(\n\u001b[1;32m    225\u001b[0m             cache_dict, \u001b[38;5;28mself\u001b[39m, has_batch_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    226\u001b[0m         )\n",
      "File \u001b[0;32m~/projects/vitsearch/.venv/lib/python3.11/site-packages/vit_prisma/prisma_tools/hooked_root_module.py:229\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m cache_dict, fwd, bwd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    226\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[38;5;241m=\u001b[39mremove_batch_dim\n\u001b[1;32m    227\u001b[0m )\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhooks(fwd_hooks\u001b[38;5;241m=\u001b[39mfwd, bwd_hooks\u001b[38;5;241m=\u001b[39mbwd, reset_hooks_end\u001b[38;5;241m=\u001b[39mreset_hooks_end, clear_contexts\u001b[38;5;241m=\u001b[39mclear_contexts):\n\u001b[0;32m--> 229\u001b[0m     model_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    231\u001b[0m         model_out\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/projects/vitsearch/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/vitsearch/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/vitsearch/.venv/lib/python3.11/site-packages/vit_prisma/models/base_vit.py:179\u001b[0m, in \u001b[0;36mHookedViT.forward\u001b[0;34m(self, input, stop_at_layer)\u001b[0m\n\u001b[1;32m    176\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_ln_pre(residual)\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[:stop_at_layer]:\n\u001b[0;32m--> 179\u001b[0m     residual \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresidual\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stop_at_layer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m residual\n",
      "File \u001b[0;32m~/projects/vitsearch/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/vitsearch/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/vitsearch/.venv/lib/python3.11/site-packages/vit_prisma/models/layers/transformer_block.py:108\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre)\u001b[0m\n\u001b[1;32m    103\u001b[0m     key_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    104\u001b[0m     value_input \u001b[38;5;241m=\u001b[39m attn_in\n\u001b[1;32m    106\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\n\u001b[1;32m    107\u001b[0m         query_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(query_input),\n\u001b[0;32m--> 108\u001b[0m         key_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_input\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    109\u001b[0m         value_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1(value_input),\n\u001b[1;32m    110\u001b[0m     )\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# Take hook fn\u001b[39;00m\n\u001b[1;32m    114\u001b[0m attn_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_attn_out(\n\u001b[1;32m    115\u001b[0m     attn_out\n\u001b[1;32m    116\u001b[0m )\n",
      "File \u001b[0;32m~/projects/vitsearch/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/vitsearch/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/vitsearch/.venv/lib/python3.11/site-packages/vit_prisma/models/layers/layer_norm.py:93\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m scale: Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch pos 1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_scale(\n\u001b[1;32m     90\u001b[0m     (x\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\u001b[38;5;241m.\u001b[39msqrt()\n\u001b[1;32m     91\u001b[0m )\n\u001b[1;32m     92\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m/\u001b[39m scale  \u001b[38;5;66;03m# [batch, pos, length]\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhook_normalized\u001b[49m(x \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/projects/vitsearch/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1716\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1714\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1716\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1718\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "top_activations = analyze_feature_activations(model, sparse_autoencoder, activation_store.image_dataloader_eval, sparse_autoencoder.W_enc, sparse_autoencoder.b_enc, k=5, device=config.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
