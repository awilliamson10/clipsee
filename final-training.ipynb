{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
    "\n",
    "@dataclass\n",
    "class SAETrainConfig(LanguageModelSAERunnerConfig):\n",
    "    dataset_path: str = 'imagenet_data'\n",
    "    num_workers: int = 0\n",
    "    num_epochs: int = 3\n",
    "\n",
    "    expansion_factor: int = 24\n",
    "    context_size: int = 257\n",
    "    d_in: int = 1024\n",
    "    model_name: str = \"laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K\"\n",
    "    hook_point: str = \"blocks.{layer}.hook_mlp_out\"\n",
    "    hook_point_layer: List[int] = field(default_factory=lambda: [22])\n",
    "    dead_feature_window: int = 2500\n",
    "    use_ghost_grads: bool = True\n",
    "    feature_sampling_window: int = 250\n",
    "    from_pretrained_path: str = None\n",
    "\n",
    "    b_dec_init_method: str = \"geometric_median\"\n",
    "    normalize_sae_decoder: bool = True\n",
    "\n",
    "    lr: float = 0.0005\n",
    "    l1_coefficient: int = 0.006\n",
    "    lr_scheduler_name: str = \"cosineannealing\"\n",
    "    train_batch_size_tokens: int = 8\n",
    "    lr_warm_up_steps: int = 4000\n",
    "\n",
    "    n_batches_in_buffer: int = 8\n",
    "    store_batch_size: int = 4\n",
    "\n",
    "    log_to_wandb: bool = True\n",
    "    wandb_project: str = \"openclip_sae_training\"\n",
    "    wandb_entity: str = \"willfulbytes\"\n",
    "    wandb_log_frequency: int = 25\n",
    "    eval_every_n_wandb_logs: int = 10\n",
    "    run_name: str = None\n",
    "\n",
    "    device: str = \"cuda\"\n",
    "    seed: int = 42\n",
    "    n_checkpoints: int = 10\n",
    "    checkpoint_path: str = \"checkpoints24\"\n",
    "    dtype: str = \"torch.float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f0d4a26e0e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from vit_prisma.models.base_vit import HookedViT\n",
    "from open_clip import tokenize\n",
    "import datasets\n",
    "from typing import Any, Iterator, cast\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class HFDataset(Dataset):\n",
    "    def __init__(self, data_location, transforms, image_col, text_col):\n",
    "        self.dataset = datasets.load_dataset(data_location, split=\"train\")\n",
    "        self.image_col = image_col\n",
    "        self.text_col = text_col\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Remove the extra dimension by squeezing the tensor\n",
    "        images = self.transforms(self.dataset[idx][self.image_col], return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "        texts = tokenize([self.dataset[idx][self.text_col]])[0]\n",
    "        return images, texts\n",
    "\n",
    "# Update the collate functions accordingly\n",
    "def collate_fn(data):\n",
    "    imgs, _ = zip(*data)\n",
    "    return torch.stack(imgs, dim=0)\n",
    "\n",
    "def collate_fn_eval(data):\n",
    "    imgs, texts = zip(*data)\n",
    "    return torch.stack(imgs, dim=0), torch.stack(texts, dim=0)\n",
    "\n",
    "\n",
    "class OpenCLIPActivationsStore:\n",
    "    \"\"\"\n",
    "    Class for streaming tokens and generating and storing activations\n",
    "    while training SAEs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: SAETrainConfig,\n",
    "        model: HookedViT,\n",
    "        dataset: torch.utils.data.Dataset,\n",
    "        eval_dataset: torch.utils.data.Dataset = None,\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        self.config = config\n",
    "        assert (\n",
    "            not self.config.normalize_activations\n",
    "        ), \"Normalize activations is currently not implemented for vision, sorry!\"\n",
    "        self.normalize_activations = self.config.normalize_activations\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "\n",
    "        self.image_dataloader = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.config.store_batch_size,\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        self.image_dataloader_eval = torch.utils.data.DataLoader(\n",
    "            self.eval_dataset,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.config.store_batch_size,\n",
    "            collate_fn=collate_fn_eval,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        self.image_dataloader_iter = self.get_batch_tokens_internal()\n",
    "        self.image_dataloader_eval_iter = self.get_val_batch_tokens_internal()\n",
    "\n",
    "        self.storage_buffer = self.get_buffer(self.config.n_batches_in_buffer // 2)\n",
    "        self.dataloader = self.get_data_loader()\n",
    "\n",
    "\n",
    "    def get_batch_tokens_internal(self):\n",
    "        \"\"\"\n",
    "        Streams a batch of tokens from a dataset.\n",
    "        \"\"\"\n",
    "        device = self.config.device\n",
    "        while True:\n",
    "            for data in self.image_dataloader:\n",
    "                data.requires_grad_(False)\n",
    "                yield data.to(device)  # 5\n",
    "\n",
    "    def get_batch_tokens(self):\n",
    "        return next(self.image_dataloader_iter)\n",
    "\n",
    "    # returns the ground truth class as well.\n",
    "    def get_val_batch_tokens_internal(self):\n",
    "        \"\"\"\n",
    "        Streams a batch of tokens from a dataset.\n",
    "        \"\"\"\n",
    "        device = self.config.device\n",
    "        while True:\n",
    "            for image_data, labels in self.image_dataloader_eval:\n",
    "                image_data.requires_grad_(False)\n",
    "                labels.requires_grad_(False)\n",
    "                yield image_data.to(device), labels.to(device)\n",
    "\n",
    "    def get_val_batch_tokens(self):\n",
    "        return next(self.image_dataloader_eval_iter)\n",
    "\n",
    "    def get_activations(self, batch_tokens: torch.Tensor, get_loss: bool = False):\n",
    "        \"\"\"\n",
    "        Returns activations of shape (batches, context, num_layers, d_in)\n",
    "        \"\"\"\n",
    "        layers = (\n",
    "            self.config.hook_point_layer\n",
    "            if isinstance(self.config.hook_point_layer, list)\n",
    "            else [self.config.hook_point_layer]\n",
    "        )\n",
    "        act_names = [self.config.hook_point.format(layer=layer) for layer in layers]\n",
    "        hook_point_max_layer = max(layers)\n",
    "\n",
    "        if self.config.hook_point_head_index is not None:\n",
    "            layerwise_activations = self.model.run_with_cache(\n",
    "                batch_tokens,\n",
    "                names_filter=act_names,\n",
    "                stop_at_layer=hook_point_max_layer + 1,\n",
    "            )[1]\n",
    "            activations_list = [\n",
    "                layerwise_activations[act_name][:, :, self.config.hook_point_head_index]\n",
    "                for act_name in act_names\n",
    "            ]\n",
    "        else:\n",
    "            layerwise_activations = self.model.run_with_cache(  ####\n",
    "                batch_tokens,\n",
    "                names_filter=act_names,\n",
    "                stop_at_layer=hook_point_max_layer + 1,\n",
    "            )[1]\n",
    "            activations_list = [\n",
    "                layerwise_activations[act_name] for act_name in act_names\n",
    "            ]\n",
    "\n",
    "        # Stack along a new dimension to keep separate layers distinct\n",
    "        stacked_activations = torch.stack(activations_list, dim=2)\n",
    "\n",
    "        return stacked_activations\n",
    "\n",
    "    def get_buffer(self, n_batches_in_buffer: int):\n",
    "        context_size = self.config.context_size\n",
    "        batch_size = self.config.store_batch_size\n",
    "        d_in = self.config.d_in\n",
    "        total_size = batch_size * n_batches_in_buffer\n",
    "        num_layers = (\n",
    "            len(self.config.hook_point_layer)\n",
    "            if isinstance(self.config.hook_point_layer, list)\n",
    "            else 1\n",
    "        )  # Number of hook points or layers\n",
    "\n",
    "        refill_iterator = range(0, batch_size * n_batches_in_buffer, batch_size)\n",
    "        # Initialize empty tensor buffer of the maximum required size with an additional dimension for layers\n",
    "        new_buffer = torch.zeros(\n",
    "            (total_size, context_size, num_layers, d_in),\n",
    "            dtype=self.config.dtype,\n",
    "            device=self.config.device,\n",
    "        )\n",
    "\n",
    "        for refill_batch_idx_start in refill_iterator:\n",
    "            refill_batch_tokens = self.get_batch_tokens()  ######\n",
    "            refill_activations = self.get_activations(refill_batch_tokens)\n",
    "\n",
    "            new_buffer[\n",
    "                refill_batch_idx_start : refill_batch_idx_start + batch_size, ...\n",
    "            ] = refill_activations\n",
    "\n",
    "            # pbar.update(1)\n",
    "\n",
    "        new_buffer = new_buffer.reshape(-1, num_layers, d_in)\n",
    "        new_buffer = new_buffer[torch.randperm(new_buffer.shape[0])]\n",
    "\n",
    "        return new_buffer\n",
    "\n",
    "    def get_data_loader(\n",
    "        self,\n",
    "    ) -> Iterator[Any]:\n",
    "        \"\"\"\n",
    "        Return a torch.utils.dataloader which you can get batches from.\n",
    "\n",
    "        Should automatically refill the buffer when it gets to n % full.\n",
    "        (better mixing if you refill and shuffle regularly).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = self.config.train_batch_size_tokens\n",
    "\n",
    "        # 1. # create new buffer by mixing stored and new buffer\n",
    "        mixing_buffer = torch.cat(\n",
    "            [self.get_buffer(self.config.n_batches_in_buffer // 2), self.storage_buffer], ####\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        mixing_buffer = mixing_buffer[torch.randperm(mixing_buffer.shape[0])]\n",
    "\n",
    "        # 2.  put 50 % in storage\n",
    "        self.storage_buffer = mixing_buffer[: mixing_buffer.shape[0] // 2]\n",
    "\n",
    "        # 3. put other 50 % in a dataloader\n",
    "        dataloader = iter(\n",
    "            DataLoader(\n",
    "                # TODO: seems like a typing bug?\n",
    "                cast(Any, mixing_buffer[mixing_buffer.shape[0] // 2 :]),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "        Get the next batch from the current DataLoader.\n",
    "        If the DataLoader is exhausted, refill the buffer and create a new DataLoader.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Try to get the next batch\n",
    "            return next(self.dataloader)\n",
    "        except StopIteration:\n",
    "            # If the DataLoader is exhausted, create a new one\n",
    "            self.dataloader = self.get_data_loader() #### 97\n",
    "            return next(self.dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 24576-L1-0.006-LR-0.0005-Tokens-2.000e+06\n",
      "n_tokens_per_buffer (millions): 0.065792\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.000256\n",
      "Total training steps: 250000\n",
      "Total wandb updates: 10000\n",
      "n_tokens_per_feature_sampling_window (millions): 0.514\n",
      "n_tokens_per_dead_feature_window (millions): 5.14\n",
      "We will reset the sparsity calculation 1000 times.\n",
      "Number tokens in sparsity calculation window: 2.00e+03\n",
      "Using Ghost Grads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/clipsee/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 24576-L1-0.006-LR-0.0005-Tokens-4.024e+04\n",
      "n_tokens_per_buffer (millions): 0.065792\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.000256\n",
      "Total training steps: 5029\n",
      "Total wandb updates: 201\n",
      "n_tokens_per_feature_sampling_window (millions): 0.514\n",
      "n_tokens_per_dead_feature_window (millions): 5.14\n",
      "We will reset the sparsity calculation 20 times.\n",
      "Number tokens in sparsity calculation window: 2.00e+03\n",
      "Using Ghost Grads.\n",
      "Run name: 24576-L1-0.006-LR-0.0005-Tokens-4.024e+04\n",
      "n_tokens_per_buffer (millions): 0.065792\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.000256\n",
      "Total training steps: 5029\n",
      "Total wandb updates: 201\n",
      "n_tokens_per_feature_sampling_window (millions): 0.514\n",
      "n_tokens_per_dead_feature_window (millions): 5.14\n",
      "We will reset the sparsity calculation 20 times.\n",
      "Number tokens in sparsity calculation window: 2.00e+03\n",
      "Using Ghost Grads.\n",
      "{'n_layers': 24, 'd_model': 1024, 'd_head': 64, 'model_name': '', 'n_heads': 16, 'd_mlp': 4096, 'activation_name': 'gelu', 'eps': 1e-05, 'original_architecture': 'vit_clip_vision_encoder', 'initializer_range': 0.02, 'n_channels': 3, 'patch_size': 14, 'image_size': 224, 'n_classes': 768, 'n_params': None, 'layer_norm_pre': True, 'return_type': 'class_logits'}\n",
      "LayerNorm folded.\n",
      "Centered weights writing to residual stream\n",
      "Loaded pretrained model laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K into HookedTransformer\n",
      "0: Name: laion_CLIP-ViT-L-14-DataComp.XL-s13B-b90K_blocks.22.hook_mlp_out_24576_hook_point_layer_22 Layer 22, p_norm 1, alpha 0.006\n"
     ]
    }
   ],
   "source": [
    "from sae_lens.training.sae_group import SparseAutoencoderDictionary\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "config = SAETrainConfig()\n",
    "processor = CLIPProcessor.from_pretrained(config.model_name)\n",
    "dataset = HFDataset(\"awilliamson/fashion-train\", processor.image_processor, \"image\", \"text\") # load_dataset(\"awilliamson/fashion-train\", split=\"train\")\n",
    "eval_dataset = HFDataset(\"awilliamson/fashion-eval\", processor.image_processor, \"image\", \"text\") # load_dataset(\"awilliamson/fashion-validation\", split=\"train\")\n",
    "# cfg.training_tokens = int(1_300_000*setup_args['num_epochs']) * cfg.context_size\n",
    "config.training_tokens = len(dataset) * config.num_epochs\n",
    "sae_group = SparseAutoencoderDictionary(config)\n",
    "model = HookedViT.from_pretrained(config.model_name, is_timm=False, is_clip=True)\n",
    "model.to(config.device)\n",
    "\n",
    "activation_store = OpenCLIPActivationsStore(\n",
    "    config = config,\n",
    "    model = model,\n",
    "    dataset = dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    num_workers = 0,\n",
    ")\n",
    "\n",
    "for i, (name, sae) in enumerate(sae_group):\n",
    "    hyp = sae.cfg\n",
    "    print(\n",
    "        f\"{i}: Name: {name} Layer {hyp.hook_point_layer}, p_norm {hyp.lp_norm}, alpha {hyp.l1_coefficient}\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwillfulbytes\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/clipsee/wandb/run-20240811_003757-1booo7jp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/willfulbytes/openclip_sae_training/runs/1booo7jp' target=\"_blank\">24576-L1-0.006-LR-0.0005-Tokens-2.000e+06</a></strong> to <a href='https://wandb.ai/willfulbytes/openclip_sae_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/willfulbytes/openclip_sae_training' target=\"_blank\">https://wandb.ai/willfulbytes/openclip_sae_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/willfulbytes/openclip_sae_training/runs/1booo7jp' target=\"_blank\">https://wandb.ai/willfulbytes/openclip_sae_training/runs/1booo7jp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Objective value: 34442.7500:   5%|▌         | 5/100 [00:00<00:00, 180.81it/s]\n",
      "/root/clipsee/.venv/lib/python3.10/site-packages/sae_lens/training/sparse_autoencoder.py:279: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.tensor(origin, dtype=self.dtype, device=self.device)\n",
      "/root/clipsee/.venv/lib/python3.10/site-packages/sae_lens/training/train_sae_on_language_model.py:611: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=autocast)\n",
      "5000| MSE Loss 7.742 | L1 0.867: : 40240it [01:58, 466.64it/s]                         /root/clipsee/.venv/lib/python3.10/site-packages/wandb/sdk/wandb_run.py:2333: UserWarning: Run (1booo7jp) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
      "  lambda data: self._console_raw_callback(\"stderr\", data),\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45a8e50a96b4dcebd2022403dc8dec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='2115.303 MB of 2115.313 MB uploaded (0.022 MB deduped)\\r'), FloatProgress(value=0.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B sync reduced upload amount by 0.0%"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient_layer22</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>details/current_learning_rate_layer22</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇█████▆▅▄▃▂▂</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>losses/ghost_grad_loss_layer22</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██</td></tr><tr><td>losses/l1_loss_layer22</td><td>█████▇▆▆▆▄▃▃▂▃▃▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂</td></tr><tr><td>losses/mse_loss_layer22</td><td>█▄▃▃▃▂▂▂▂▂▁▂▂▃▅▂▃▂▂▁▂▁▁▂▂▂▃▂▁▂▂▂▃▁▂▁▁▁▁▁</td></tr><tr><td>losses/overall_loss_layer22</td><td>█▄▃▃▃▂▂▂▂▂▁▂▂▂▄▂▂▂▂▁▂▁▁▂▂▂▃▂▁▁▂▂▂▁▂▁▁▁▁▁</td></tr><tr><td>metrics/explained_variance_layer22</td><td>▁▃▅▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>metrics/explained_variance_std_layer22</td><td>█▆▄▄▃▃▂▂▃▂▁▂▂▂▃▁▁▂▃▁▂▂▁▂▁▂▅▃▂▂▂▃▃▁▁▂▁▂▁▁</td></tr><tr><td>metrics/l0_layer22</td><td>█████▇▇▇▇▆▄▄▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l2_norm_layer22</td><td>▃▅▇▇▅▂▃▄▃▁▆▄▆▃█▅▁▇▄▆</td></tr><tr><td>metrics/l2_ratio_layer22</td><td>▃▅▇█▆▂▃▆▂▁▃▄▄▄▅▄▂▅▅▆</td></tr><tr><td>metrics/mean_log10_feature_sparsity_layer22</td><td>██████▇▇▇▇▆▆▅▄▄▃▂▂▁▁</td></tr><tr><td>sparsity/below_1e-5_layer22</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▂▃▄▅▆▇▇█</td></tr><tr><td>sparsity/below_1e-6_layer22</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▂▃▄▅▆▇▇█</td></tr><tr><td>sparsity/dead_features_layer22</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂█</td></tr><tr><td>sparsity/mean_passes_since_fired_layer22</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▄▄▄▅▅▆▆▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_l1_coefficient_layer22</td><td>0.006</td></tr><tr><td>details/current_learning_rate_layer22</td><td>5e-05</td></tr><tr><td>details/n_training_tokens</td><td>40200</td></tr><tr><td>losses/ghost_grad_loss_layer22</td><td>0.00621</td></tr><tr><td>losses/l1_loss_layer22</td><td>129.13976</td></tr><tr><td>losses/mse_loss_layer22</td><td>6.37519</td></tr><tr><td>losses/overall_loss_layer22</td><td>7.15624</td></tr><tr><td>metrics/explained_variance_layer22</td><td>0.92948</td></tr><tr><td>metrics/explained_variance_std_layer22</td><td>0.01954</td></tr><tr><td>metrics/l0_layer22</td><td>782.25</td></tr><tr><td>metrics/l2_norm_layer22</td><td>10.46005</td></tr><tr><td>metrics/l2_ratio_layer22</td><td>0.97426</td></tr><tr><td>metrics/mean_log10_feature_sparsity_layer22</td><td>-7.04752</td></tr><tr><td>sparsity/below_1e-5_layer22</td><td>15065</td></tr><tr><td>sparsity/below_1e-6_layer22</td><td>15065</td></tr><tr><td>sparsity/dead_features_layer22</td><td>36</td></tr><tr><td>sparsity/mean_passes_since_fired_layer22</td><td>629.95312</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">24576-L1-0.006-LR-0.0005-Tokens-2.000e+06</strong> at: <a href='https://wandb.ai/willfulbytes/openclip_sae_training/runs/1booo7jp' target=\"_blank\">https://wandb.ai/willfulbytes/openclip_sae_training/runs/1booo7jp</a><br/> View project at: <a href='https://wandb.ai/willfulbytes/openclip_sae_training' target=\"_blank\">https://wandb.ai/willfulbytes/openclip_sae_training</a><br/>Synced 5 W&B file(s), 0 media file(s), 44 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240811_003757-1booo7jp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "torch.set_grad_enabled(True)\n",
    "from sae.train import train_sae_group_on_vision_model\n",
    "\n",
    "\n",
    "if config.log_to_wandb:\n",
    "    wandb.init(project=config.wandb_project, config=cast(Any, config), name=config.run_name)\n",
    "\n",
    "train_sae_group_on_vision_model(\n",
    "    model,\n",
    "    sae_group,\n",
    "    activation_store,\n",
    "    train_contexts=None, #TODO load checkpoints correctly to match saelens v2.1.3 lm_runner!\n",
    "    training_run_state=None,  #TODO load checkpoints correctly to match saelens v2.1.3 lm_runner!\n",
    "    n_checkpoints=config.n_checkpoints,\n",
    "    batch_size=config.train_batch_size_tokens,\n",
    "    feature_sampling_window=config.feature_sampling_window,\n",
    "    use_wandb=config.log_to_wandb,\n",
    "    wandb_log_frequency=config.wandb_log_frequency,\n",
    "    eval_every_n_wandb_logs=config.eval_every_n_wandb_logs,\n",
    "    autocast=config.autocast,\n",
    ")\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
