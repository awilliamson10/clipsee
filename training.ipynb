{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
    "\n",
    "@dataclass\n",
    "class SAETrainConfig(LanguageModelSAERunnerConfig):\n",
    "    dataset_path: str = 'imagenet_data'\n",
    "    num_workers: int = 0\n",
    "    num_epochs: int = 1\n",
    "\n",
    "    expansion_factor: int = 32\n",
    "    context_size: int = 257\n",
    "    d_in: int = 1024\n",
    "    model_name: str = \"laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K\"\n",
    "    model_base: str = \"ViT-L-14\"\n",
    "    model_pretrained: str = \"datacomp_xl_s13b_b90k\"\n",
    "    hook_point: str = \"blocks.{layer}.hook_mlp_out\"\n",
    "    hook_point_layer: List[int] = field(default_factory=lambda: [23])\n",
    "    dead_feature_window: int = 5000\n",
    "    use_ghost_grads: bool = True\n",
    "    feature_sampling_window: int = 1000\n",
    "    from_pretrained_path: str = None\n",
    "\n",
    "    b_dec_init_method: str = \"geometric_median\"\n",
    "\n",
    "    lr: float = 0.0004\n",
    "    l1_coefficient: float = 0.00008\n",
    "    lr_scheduler_name: str = \"constant\"\n",
    "    train_batch_size_tokens: int = 4\n",
    "    lr_warm_up_steps: int = 5000\n",
    "\n",
    "    n_batches_in_buffer: int = 4\n",
    "    store_batch_size: int = 4\n",
    "\n",
    "    log_to_wandb: bool = True\n",
    "    wandb_project: str = \"openclip_sae_training\"\n",
    "    wandb_entity: str = \"willfulbytes\"\n",
    "    wandb_log_frequency: int = 100\n",
    "    eval_every_n_wandb_logs: int = 10\n",
    "    run_name: str = None\n",
    "\n",
    "    device: str = \"mps\"\n",
    "    seed: int = 42\n",
    "    n_checkpoints: int = 10\n",
    "    checkpoint_path: str = None\n",
    "    dtype: str = \"torch.float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x106021950>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from vit_prisma.models.base_vit import HookedViT\n",
    "from open_clip import tokenize\n",
    "import datasets\n",
    "from typing import Any, Iterator, cast\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class HFDataset(Dataset):\n",
    "    def __init__(self, data_location, transforms, image_col, text_col):\n",
    "        self.dataset = datasets.load_dataset(data_location, split=\"train\")\n",
    "        self.image_col = image_col\n",
    "        self.text_col = text_col\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Remove the extra dimension by squeezing the tensor\n",
    "        images = self.transforms(self.dataset[idx][self.image_col], return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "        texts = tokenize([self.dataset[idx][self.text_col]])[0]\n",
    "        return images, texts\n",
    "\n",
    "# Update the collate functions accordingly\n",
    "def collate_fn(data):\n",
    "    imgs, _ = zip(*data)\n",
    "    return torch.stack(imgs, dim=0)\n",
    "\n",
    "def collate_fn_eval(data):\n",
    "    imgs, texts = zip(*data)\n",
    "    return torch.stack(imgs, dim=0), torch.stack(texts, dim=0)\n",
    "\n",
    "\n",
    "class OpenCLIPActivationsStore:\n",
    "    \"\"\"\n",
    "    Class for streaming tokens and generating and storing activations\n",
    "    while training SAEs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: SAETrainConfig,\n",
    "        model: HookedViT,\n",
    "        dataset: torch.utils.data.Dataset,\n",
    "        eval_dataset: torch.utils.data.Dataset = None,\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        self.config = config\n",
    "        assert (\n",
    "            not self.config.normalize_activations\n",
    "        ), \"Normalize activations is currently not implemented for vision, sorry!\"\n",
    "        self.normalize_activations = self.config.normalize_activations\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "\n",
    "        self.image_dataloader = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.config.store_batch_size,\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        self.image_dataloader_eval = torch.utils.data.DataLoader(\n",
    "            self.eval_dataset,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.config.store_batch_size,\n",
    "            collate_fn=collate_fn_eval,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        self.image_dataloader_iter = self.get_batch_tokens_internal()\n",
    "        self.image_dataloader_eval_iter = self.get_val_batch_tokens_internal()\n",
    "\n",
    "        self.storage_buffer = self.get_buffer(self.config.n_batches_in_buffer // 2)\n",
    "        self.dataloader = self.get_data_loader()\n",
    "\n",
    "\n",
    "    def get_batch_tokens_internal(self):\n",
    "        \"\"\"\n",
    "        Streams a batch of tokens from a dataset.\n",
    "        \"\"\"\n",
    "        device = self.config.device\n",
    "        while True:\n",
    "            for data in self.image_dataloader:\n",
    "                data.requires_grad_(False)\n",
    "                yield data.to(device)  # 5\n",
    "\n",
    "    def get_batch_tokens(self):\n",
    "        return next(self.image_dataloader_iter)\n",
    "\n",
    "    # returns the ground truth class as well.\n",
    "    def get_val_batch_tokens_internal(self):\n",
    "        \"\"\"\n",
    "        Streams a batch of tokens from a dataset.\n",
    "        \"\"\"\n",
    "        device = self.config.device\n",
    "        while True:\n",
    "            for image_data, labels in self.image_dataloader_eval:\n",
    "                image_data.requires_grad_(False)\n",
    "                labels.requires_grad_(False)\n",
    "                yield image_data.to(device), labels.to(device)\n",
    "\n",
    "    def get_val_batch_tokens(self):\n",
    "        return next(self.image_dataloader_eval_iter)\n",
    "\n",
    "    def get_activations(self, batch_tokens: torch.Tensor, get_loss: bool = False):\n",
    "        \"\"\"\n",
    "        Returns activations of shape (batches, context, num_layers, d_in)\n",
    "        \"\"\"\n",
    "        layers = (\n",
    "            self.config.hook_point_layer\n",
    "            if isinstance(self.config.hook_point_layer, list)\n",
    "            else [self.config.hook_point_layer]\n",
    "        )\n",
    "        act_names = [self.config.hook_point.format(layer=layer) for layer in layers]\n",
    "        hook_point_max_layer = max(layers)\n",
    "\n",
    "        if self.config.hook_point_head_index is not None:\n",
    "            layerwise_activations = self.model.run_with_cache(\n",
    "                batch_tokens,\n",
    "                names_filter=act_names,\n",
    "                stop_at_layer=hook_point_max_layer + 1,\n",
    "            )[1]\n",
    "            activations_list = [\n",
    "                layerwise_activations[act_name][:, :, self.config.hook_point_head_index]\n",
    "                for act_name in act_names\n",
    "            ]\n",
    "        else:\n",
    "            layerwise_activations = self.model.run_with_cache(  ####\n",
    "                batch_tokens,\n",
    "                names_filter=act_names,\n",
    "                stop_at_layer=hook_point_max_layer + 1,\n",
    "            )[1]\n",
    "            activations_list = [\n",
    "                layerwise_activations[act_name] for act_name in act_names\n",
    "            ]\n",
    "\n",
    "        # Stack along a new dimension to keep separate layers distinct\n",
    "        stacked_activations = torch.stack(activations_list, dim=2)\n",
    "\n",
    "        return stacked_activations\n",
    "\n",
    "    def get_buffer(self, n_batches_in_buffer: int):\n",
    "        context_size = self.config.context_size\n",
    "        batch_size = self.config.store_batch_size\n",
    "        d_in = self.config.d_in\n",
    "        total_size = batch_size * n_batches_in_buffer\n",
    "        num_layers = (\n",
    "            len(self.config.hook_point_layer)\n",
    "            if isinstance(self.config.hook_point_layer, list)\n",
    "            else 1\n",
    "        )  # Number of hook points or layers\n",
    "\n",
    "        refill_iterator = range(0, batch_size * n_batches_in_buffer, batch_size)\n",
    "        # Initialize empty tensor buffer of the maximum required size with an additional dimension for layers\n",
    "        new_buffer = torch.zeros(\n",
    "            (total_size, context_size, num_layers, d_in),\n",
    "            dtype=self.config.dtype,\n",
    "            device=self.config.device,\n",
    "        )\n",
    "\n",
    "        for refill_batch_idx_start in refill_iterator:\n",
    "            refill_batch_tokens = self.get_batch_tokens()  ######\n",
    "            refill_activations = self.get_activations(refill_batch_tokens)\n",
    "\n",
    "            new_buffer[\n",
    "                refill_batch_idx_start : refill_batch_idx_start + batch_size, ...\n",
    "            ] = refill_activations\n",
    "\n",
    "            # pbar.update(1)\n",
    "\n",
    "        new_buffer = new_buffer.reshape(-1, num_layers, d_in)\n",
    "        new_buffer = new_buffer[torch.randperm(new_buffer.shape[0])]\n",
    "\n",
    "        return new_buffer\n",
    "\n",
    "    def get_data_loader(\n",
    "        self,\n",
    "    ) -> Iterator[Any]:\n",
    "        \"\"\"\n",
    "        Return a torch.utils.dataloader which you can get batches from.\n",
    "\n",
    "        Should automatically refill the buffer when it gets to n % full.\n",
    "        (better mixing if you refill and shuffle regularly).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = self.config.train_batch_size_tokens\n",
    "\n",
    "        # 1. # create new buffer by mixing stored and new buffer\n",
    "        mixing_buffer = torch.cat(\n",
    "            [self.get_buffer(self.config.n_batches_in_buffer // 2), self.storage_buffer], ####\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        mixing_buffer = mixing_buffer[torch.randperm(mixing_buffer.shape[0])]\n",
    "\n",
    "        # 2.  put 50 % in storage\n",
    "        self.storage_buffer = mixing_buffer[: mixing_buffer.shape[0] // 2]\n",
    "\n",
    "        # 3. put other 50 % in a dataloader\n",
    "        dataloader = iter(\n",
    "            DataLoader(\n",
    "                # TODO: seems like a typing bug?\n",
    "                cast(Any, mixing_buffer[mixing_buffer.shape[0] // 2 :]),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "        Get the next batch from the current DataLoader.\n",
    "        If the DataLoader is exhausted, refill the buffer and create a new DataLoader.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Try to get the next batch\n",
    "            return next(self.dataloader)\n",
    "        except StopIteration:\n",
    "            # If the DataLoader is exhausted, create a new one\n",
    "            self.dataloader = self.get_data_loader() #### 97\n",
    "            return next(self.dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 32768-L1-8e-05-LR-0.0004-Tokens-2.000e+06\n",
      "n_tokens_per_buffer (millions): 0.032896\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.000128\n",
      "Total training steps: 500000\n",
      "Total wandb updates: 5000\n",
      "n_tokens_per_feature_sampling_window (millions): 1.028\n",
      "n_tokens_per_dead_feature_window (millions): 5.14\n",
      "We will reset the sparsity calculation 500 times.\n",
      "Number tokens in sparsity calculation window: 4.00e+03\n",
      "Using Ghost Grads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aw/projects/vitsearch/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 32768-L1-8e-05-LR-0.0004-Tokens-3.447e+06\n",
      "n_tokens_per_buffer (millions): 0.032896\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.000128\n",
      "Total training steps: 861785\n",
      "Total wandb updates: 8617\n",
      "n_tokens_per_feature_sampling_window (millions): 1.028\n",
      "n_tokens_per_dead_feature_window (millions): 5.14\n",
      "We will reset the sparsity calculation 861 times.\n",
      "Number tokens in sparsity calculation window: 4.00e+03\n",
      "Using Ghost Grads.\n",
      "Run name: 32768-L1-8e-05-LR-0.0004-Tokens-3.447e+06\n",
      "n_tokens_per_buffer (millions): 0.032896\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.000128\n",
      "Total training steps: 861785\n",
      "Total wandb updates: 8617\n",
      "n_tokens_per_feature_sampling_window (millions): 1.028\n",
      "n_tokens_per_dead_feature_window (millions): 5.14\n",
      "We will reset the sparsity calculation 861 times.\n",
      "Number tokens in sparsity calculation window: 4.00e+03\n",
      "Using Ghost Grads.\n",
      "{'n_layers': 24, 'd_model': 1024, 'd_head': 64, 'model_name': '', 'n_heads': 16, 'd_mlp': 4096, 'activation_name': 'gelu', 'eps': 1e-05, 'original_architecture': 'vit_clip_vision_encoder', 'initializer_range': 0.02, 'n_channels': 3, 'patch_size': 14, 'image_size': 224, 'n_classes': 768, 'n_params': None, 'layer_norm_pre': True, 'return_type': 'class_logits'}\n",
      "LayerNorm folded.\n",
      "Centered weights writing to residual stream\n",
      "Loaded pretrained model laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K into HookedTransformer\n",
      "0: Name: laion_CLIP-ViT-L-14-DataComp.XL-s13B-b90K_blocks.23.hook_mlp_out_32768_hook_point_layer_23 Layer 23, p_norm 1, alpha 8e-05\n"
     ]
    }
   ],
   "source": [
    "from sae_lens.training.sae_group import SparseAutoencoderDictionary\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "config = SAETrainConfig()\n",
    "processor = CLIPProcessor.from_pretrained(config.model_name)\n",
    "dataset = HFDataset(\"awilliamson/fashion-train\", processor.image_processor, \"image\", \"text\") # load_dataset(\"awilliamson/fashion-train\", split=\"train\")\n",
    "eval_dataset = HFDataset(\"awilliamson/fashion-eval\", processor.image_processor, \"image\", \"text\") # load_dataset(\"awilliamson/fashion-validation\", split=\"train\")\n",
    "# cfg.training_tokens = int(1_300_000*setup_args['num_epochs']) * cfg.context_size\n",
    "config.training_tokens = len(dataset) * config.context_size * config.num_epochs\n",
    "sae_group = SparseAutoencoderDictionary(config)\n",
    "model = HookedViT.from_pretrained(config.model_name, is_timm=False, is_clip=True)\n",
    "model.to(config.device)\n",
    "\n",
    "activation_store = OpenCLIPActivationsStore(\n",
    "    config = config,\n",
    "    model = model,\n",
    "    dataset = dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    num_workers = 0,\n",
    ")\n",
    "\n",
    "for i, (name, sae) in enumerate(sae_group):\n",
    "    hyp = sae.cfg\n",
    "    print(\n",
    "        f\"{i}: Name: {name} Layer {hyp.hook_point_layer}, p_norm {hyp.lp_norm}, alpha {hyp.l1_coefficient}\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen layer 23 hook point blocks.23.hook_mlp_out\n"
     ]
    }
   ],
   "source": [
    "AUTOENCODER_NAME = \"laion_CLIP-ViT-L-14-DataComp.XL-s13B-b90K_blocks.23.hook_mlp_out_16384_layers_23\"\n",
    "sparse_autoencoder = sae_group.autoencoders[AUTOENCODER_NAME]\n",
    "sparse_autoencoder = sparse_autoencoder.to(config.device)\n",
    "layer_num = sparse_autoencoder.cfg.hook_point_layer\n",
    "print(f\"Chosen layer {layer_num} hook point {sparse_autoencoder.cfg.hook_point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average l0 8204.2265625\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "bingroup": "x",
         "hovertemplate": "variable=0<br>value=%{x}<br>count=%{y}<extra></extra>",
         "legendgroup": "0",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "0",
         "offsetgroup": "0",
         "orientation": "v",
         "showlegend": true,
         "type": "histogram",
         "x": [
          8289,
          8223,
          8245,
          8206,
          8233,
          8193,
          8206,
          8188,
          8149,
          8203,
          8173,
          8216,
          8229,
          8235,
          8231,
          8249,
          8236,
          8186,
          8191,
          8179,
          8259,
          8224,
          8288,
          8219,
          8192,
          8180,
          8211,
          8234,
          8238,
          8199,
          8216,
          8219,
          8227,
          8206,
          8203,
          8216,
          8209,
          8269,
          8204,
          8186,
          8197,
          8175,
          8266,
          8173,
          8193,
          8215,
          8199,
          8202,
          8267,
          8229,
          8194,
          8205,
          8267,
          8145,
          8198,
          8166,
          8193,
          8223,
          8209,
          8224,
          8203,
          8249,
          8200,
          8209,
          8224,
          8226,
          8224,
          8212,
          8191,
          8180,
          8219,
          8216,
          8211,
          8237,
          8169,
          8220,
          8210,
          8200,
          8160,
          8244,
          8225,
          8194,
          8217,
          8201,
          8211,
          8174,
          8183,
          8234,
          8240,
          8202,
          8213,
          8192,
          8174,
          8219,
          8200,
          8238,
          8151,
          8240,
          8160,
          8217,
          8227,
          8196,
          8241,
          8233,
          8161,
          8211,
          8183,
          8183,
          8204,
          8170,
          8212,
          8198,
          8224,
          8191,
          8209,
          8204,
          8180,
          8160,
          8176,
          8308,
          8181,
          8219,
          8236,
          8189,
          8223,
          8238,
          8227,
          8180,
          8198,
          8162,
          8175,
          8173,
          8176,
          8229,
          8173,
          8196,
          8184,
          8202,
          8171,
          8198,
          8217,
          8194,
          8259,
          8197,
          8180,
          8206,
          8230,
          8193,
          8214,
          8185,
          8168,
          8167,
          8176,
          8244,
          8220,
          8228,
          8169,
          8228,
          8178,
          8283,
          8154,
          8169,
          8200,
          8155,
          8160,
          8178,
          8134,
          8220,
          8193,
          8170,
          8192,
          8203,
          8174,
          8171,
          8235,
          8190,
          8185,
          8218,
          8178,
          8203,
          8233,
          8196,
          8211,
          8267,
          8217,
          8193,
          8149,
          8200,
          8246,
          8185,
          8144,
          8174,
          8172,
          8250,
          8169,
          8190,
          8183,
          8176,
          8188,
          8197,
          8195,
          8151,
          8216,
          8187,
          8212,
          8183,
          8213,
          8190,
          8226,
          8224,
          8222,
          8206,
          8204,
          8224,
          8170,
          8214,
          8243,
          8172,
          8220,
          8166,
          8230,
          8202,
          8217,
          8200,
          8234,
          8210,
          8184,
          8226,
          8238,
          8222,
          8178,
          8256,
          8227,
          8209,
          8228,
          8227,
          8196,
          8188,
          8207,
          8243,
          8233,
          8251,
          8241,
          8224,
          8225,
          8202,
          8242,
          8268,
          8164,
          8216,
          8220,
          8190,
          8255,
          8227,
          8210,
          8246,
          8246,
          8210,
          8137,
          8216,
          8232,
          8214,
          8188,
          8167,
          8222,
          8261,
          8199,
          8241,
          8173,
          8253,
          8159,
          8247,
          8175,
          8172,
          8181,
          8252,
          8198,
          8217,
          8172,
          8142,
          8201,
          8200,
          8169,
          8189,
          8183,
          8198,
          8189,
          8178,
          8230,
          8181,
          8169,
          8223,
          8211,
          8202,
          8168,
          8128,
          8221,
          8193,
          8177,
          8173,
          8225,
          8161,
          8185,
          8253,
          8184,
          8240,
          8276,
          8187,
          8229,
          8165,
          8192,
          8228,
          8193,
          8193,
          8204,
          8199,
          8195,
          8279,
          8298,
          8206,
          8169,
          8195,
          8191,
          8205,
          8170,
          8218,
          8149,
          8232,
          8174,
          8161,
          8235,
          8174,
          8197,
          8223,
          8246,
          8242,
          8132,
          8188,
          8203,
          8183,
          8166,
          8169,
          8176,
          8169,
          8189,
          8186,
          8158,
          8186,
          8200,
          8231,
          8159,
          8200,
          8223,
          8129,
          8220,
          8195,
          8227,
          8224,
          8199,
          8177,
          8228,
          8242,
          8189,
          8194,
          8225,
          8201,
          8191,
          8172,
          8214,
          8195,
          8234,
          8227,
          8202,
          8196,
          8146,
          8197,
          8198,
          8208,
          8240,
          8211,
          8205,
          8208,
          8203,
          8174,
          8242,
          8188,
          8191,
          8176,
          8227,
          8226,
          8227,
          8221,
          8171,
          8159,
          8254,
          8234,
          8222,
          8191,
          8216,
          8183,
          8209,
          8218,
          8210,
          8188,
          8189,
          8288,
          8226,
          8206,
          8191,
          8264,
          8198,
          8215,
          8237,
          8177,
          8224,
          8291,
          8201,
          8206,
          8203,
          8235,
          8212,
          8248,
          8188,
          8152,
          8187,
          8275,
          8208,
          8186,
          8197,
          8177,
          8151,
          8181,
          8182,
          8236,
          8251,
          8149,
          8189,
          8168,
          8208,
          8273,
          8220,
          8198,
          8218,
          8180,
          8154,
          8217,
          8181,
          8150,
          8141,
          8156,
          8204,
          8197,
          8166,
          8248,
          8201,
          8177,
          8165,
          8213,
          8154,
          8200,
          8210,
          8206,
          8205,
          8139,
          8191,
          8215,
          8245,
          8186,
          8267,
          8198,
          8185,
          8153,
          8210,
          8194,
          8143,
          8271,
          8183,
          8220,
          8224,
          8187,
          8227,
          8239,
          8194,
          8197,
          8238,
          8213,
          8211,
          8239,
          8218,
          8204,
          8202,
          8199,
          8181,
          8176,
          8211,
          8214,
          8259,
          8170,
          8168,
          8145,
          8197,
          8228,
          8194,
          8216,
          8155,
          8256,
          8360,
          8211,
          8180,
          8222,
          8164,
          8242
         ],
         "xaxis": "x",
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "title": {
          "text": "variable"
         },
         "tracegroupgap": 0
        },
        "margin": {
         "t": 60
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "value"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "count"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "sparse_autoencoder.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "with torch.no_grad():\n",
    "    batch_tokens, labels = activation_store.get_val_batch_tokens()\n",
    "    _, cache = model.run_with_cache(batch_tokens)\n",
    "    sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(\n",
    "        cache[sparse_autoencoder.cfg.hook_point]\n",
    "    )\n",
    "    # del cache\n",
    "\n",
    "    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
    "    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
    "    print(\"average l0\", l0.mean().item())\n",
    "    px.histogram(l0.flatten().cpu().numpy()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwillfulbytes\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/aw/projects/vitsearch/wandb/run-20240810_150314-h5d7rejr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/willfulbytes/openclip_sae_training/runs/h5d7rejr' target=\"_blank\">32768-L1-8e-05-LR-0.0004-Tokens-2.000e+06</a></strong> to <a href='https://wandb.ai/willfulbytes/openclip_sae_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/willfulbytes/openclip_sae_training' target=\"_blank\">https://wandb.ai/willfulbytes/openclip_sae_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/willfulbytes/openclip_sae_training/runs/h5d7rejr' target=\"_blank\">https://wandb.ai/willfulbytes/openclip_sae_training/runs/h5d7rejr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Objective value: 11732.4385:   4%|▍         | 4/100 [00:00<00:18,  5.14it/s]\n",
      "/Users/aw/projects/vitsearch/.venv/lib/python3.11/site-packages/sae_lens/training/sparse_autoencoder.py:279: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.tensor(origin, dtype=self.dtype, device=self.device)\n",
      "/Users/aw/projects/vitsearch/.venv/lib/python3.11/site-packages/sae_lens/training/train_sae_on_language_model.py:611: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=autocast)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interrupted, saving progress\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "torch.set_grad_enabled(True)\n",
    "from sae.train import train_sae_group_on_vision_model\n",
    "\n",
    "\n",
    "if config.log_to_wandb:\n",
    "    wandb.init(project=config.wandb_project, config=cast(Any, config), name=config.run_name)\n",
    "\n",
    "train_sae_group_on_vision_model(\n",
    "    model,\n",
    "    sae_group,\n",
    "    activation_store,\n",
    "    train_contexts=None, #TODO load checkpoints correctly to match saelens v2.1.3 lm_runner!\n",
    "    training_run_state=None,  #TODO load checkpoints correctly to match saelens v2.1.3 lm_runner!\n",
    "    n_checkpoints=config.n_checkpoints,\n",
    "    batch_size=config.train_batch_size_tokens,\n",
    "    feature_sampling_window=config.feature_sampling_window,\n",
    "    use_wandb=config.log_to_wandb,\n",
    "    wandb_log_frequency=config.wandb_log_frequency,\n",
    "    eval_every_n_wandb_logs=config.eval_every_n_wandb_logs,\n",
    "    autocast=config.autocast,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
