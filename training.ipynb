{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "from sae_lens.training.config import LanguageModelSAERunnerConfig\n",
    "\n",
    "@dataclass\n",
    "class SAETrainConfig(LanguageModelSAERunnerConfig):\n",
    "    dataset_path: str = 'imagenet_data'\n",
    "    num_workers: int = 0\n",
    "    num_epochs: int = 1\n",
    "\n",
    "    expansion_factor: int = 32\n",
    "    context_size: int = 257\n",
    "    d_in: int = 1024\n",
    "    model_name: str = \"laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K\"\n",
    "    model_base: str = \"ViT-L-14\"\n",
    "    model_pretrained: str = \"datacomp_xl_s13b_b90k\"\n",
    "    hook_point: str = \"blocks.{layer}.hook_mlp_out\"\n",
    "    hook_point_layer: List[int] = field(default_factory=lambda: [23])\n",
    "    dead_feature_window: int = 5000\n",
    "    use_ghost_grads: bool = True\n",
    "    feature_sampling_window: int = 1000\n",
    "    from_pretrained_path: str = None\n",
    "\n",
    "    b_dec_init_method: str = \"geometric_median\"\n",
    "\n",
    "    lr: float = 0.0004\n",
    "    l1_coefficient: float = 0.00008\n",
    "    lr_scheduler_name: str = \"constant\"\n",
    "    train_batch_size_tokens: int = 4\n",
    "    lr_warm_up_steps: int = 5000\n",
    "\n",
    "    n_batches_in_buffer: int = 4\n",
    "    store_batch_size: int = 4\n",
    "\n",
    "    log_to_wandb: bool = True\n",
    "    wandb_project: str = \"openclip_sae_training\"\n",
    "    wandb_entity: str = \"willfulbytes\"\n",
    "    wandb_log_frequency: int = 100\n",
    "    eval_every_n_wandb_logs: int = 10\n",
    "    run_name: str = None\n",
    "\n",
    "    device: str = \"mps\"\n",
    "    seed: int = 42\n",
    "    n_checkpoints: int = 10\n",
    "    checkpoint_path: str = None\n",
    "    dtype: str = \"torch.float32\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x174606350>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from vit_prisma.models.base_vit import HookedViT\n",
    "from open_clip import tokenize\n",
    "import datasets\n",
    "from typing import Any, Iterator, cast\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class HFDataset(Dataset):\n",
    "    def __init__(self, data_location, transforms, image_col, text_col):\n",
    "        self.dataset = datasets.load_dataset(data_location, split=\"train\")\n",
    "        self.image_col = image_col\n",
    "        self.text_col = text_col\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Remove the extra dimension by squeezing the tensor\n",
    "        images = self.transforms(self.dataset[idx][self.image_col], return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "        texts = tokenize([self.dataset[idx][self.text_col]])[0]\n",
    "        return images, texts\n",
    "\n",
    "# Update the collate functions accordingly\n",
    "def collate_fn(data):\n",
    "    imgs, _ = zip(*data)\n",
    "    return torch.stack(imgs, dim=0)\n",
    "\n",
    "def collate_fn_eval(data):\n",
    "    imgs, texts = zip(*data)\n",
    "    return torch.stack(imgs, dim=0), torch.stack(texts, dim=0)\n",
    "\n",
    "\n",
    "class OpenCLIPActivationsStore:\n",
    "    \"\"\"\n",
    "    Class for streaming tokens and generating and storing activations\n",
    "    while training SAEs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: SAETrainConfig,\n",
    "        model: HookedViT,\n",
    "        dataset: torch.utils.data.Dataset,\n",
    "        eval_dataset: torch.utils.data.Dataset = None,\n",
    "        num_workers: int = 0,\n",
    "    ):\n",
    "        self.config = config\n",
    "        assert (\n",
    "            not self.config.normalize_activations\n",
    "        ), \"Normalize activations is currently not implemented for vision, sorry!\"\n",
    "        self.normalize_activations = self.config.normalize_activations\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.eval_dataset = eval_dataset\n",
    "\n",
    "        self.image_dataloader = torch.utils.data.DataLoader(\n",
    "            self.dataset,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.config.store_batch_size,\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=True,\n",
    "        )\n",
    "        self.image_dataloader_eval = torch.utils.data.DataLoader(\n",
    "            self.eval_dataset,\n",
    "            shuffle=True,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=self.config.store_batch_size,\n",
    "            collate_fn=collate_fn_eval,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "        self.image_dataloader_iter = self.get_batch_tokens_internal()\n",
    "        self.image_dataloader_eval_iter = self.get_val_batch_tokens_internal()\n",
    "\n",
    "        self.storage_buffer = self.get_buffer(self.config.n_batches_in_buffer // 2)\n",
    "        self.dataloader = self.get_data_loader()\n",
    "\n",
    "\n",
    "    def get_batch_tokens_internal(self):\n",
    "        \"\"\"\n",
    "        Streams a batch of tokens from a dataset.\n",
    "        \"\"\"\n",
    "        device = self.config.device\n",
    "        while True:\n",
    "            for data in self.image_dataloader:\n",
    "                data.requires_grad_(False)\n",
    "                yield data.to(device)  # 5\n",
    "\n",
    "    def get_batch_tokens(self):\n",
    "        return next(self.image_dataloader_iter)\n",
    "\n",
    "    # returns the ground truth class as well.\n",
    "    def get_val_batch_tokens_internal(self):\n",
    "        \"\"\"\n",
    "        Streams a batch of tokens from a dataset.\n",
    "        \"\"\"\n",
    "        device = self.config.device\n",
    "        while True:\n",
    "            for image_data, labels in self.image_dataloader_eval:\n",
    "                image_data.requires_grad_(False)\n",
    "                labels.requires_grad_(False)\n",
    "                yield image_data.to(device), labels.to(device)\n",
    "\n",
    "    def get_val_batch_tokens(self):\n",
    "        return next(self.image_dataloader_eval_iter)\n",
    "\n",
    "    def get_activations(self, batch_tokens: torch.Tensor, get_loss: bool = False):\n",
    "        \"\"\"\n",
    "        Returns activations of shape (batches, context, num_layers, d_in)\n",
    "        \"\"\"\n",
    "        layers = (\n",
    "            self.config.hook_point_layer\n",
    "            if isinstance(self.config.hook_point_layer, list)\n",
    "            else [self.config.hook_point_layer]\n",
    "        )\n",
    "        act_names = [self.config.hook_point.format(layer=layer) for layer in layers]\n",
    "        hook_point_max_layer = max(layers)\n",
    "\n",
    "        if self.config.hook_point_head_index is not None:\n",
    "            layerwise_activations = self.model.run_with_cache(\n",
    "                batch_tokens,\n",
    "                names_filter=act_names,\n",
    "                stop_at_layer=hook_point_max_layer + 1,\n",
    "            )[1]\n",
    "            activations_list = [\n",
    "                layerwise_activations[act_name][:, :, self.config.hook_point_head_index]\n",
    "                for act_name in act_names\n",
    "            ]\n",
    "        else:\n",
    "            layerwise_activations = self.model.run_with_cache(  ####\n",
    "                batch_tokens,\n",
    "                names_filter=act_names,\n",
    "                stop_at_layer=hook_point_max_layer + 1,\n",
    "            )[1]\n",
    "            activations_list = [\n",
    "                layerwise_activations[act_name] for act_name in act_names\n",
    "            ]\n",
    "\n",
    "        # Stack along a new dimension to keep separate layers distinct\n",
    "        stacked_activations = torch.stack(activations_list, dim=2)\n",
    "\n",
    "        return stacked_activations\n",
    "\n",
    "    def get_buffer(self, n_batches_in_buffer: int):\n",
    "        context_size = self.config.context_size\n",
    "        batch_size = self.config.store_batch_size\n",
    "        d_in = self.config.d_in\n",
    "        total_size = batch_size * n_batches_in_buffer\n",
    "        num_layers = (\n",
    "            len(self.config.hook_point_layer)\n",
    "            if isinstance(self.config.hook_point_layer, list)\n",
    "            else 1\n",
    "        )  # Number of hook points or layers\n",
    "\n",
    "        refill_iterator = range(0, batch_size * n_batches_in_buffer, batch_size)\n",
    "        # Initialize empty tensor buffer of the maximum required size with an additional dimension for layers\n",
    "        new_buffer = torch.zeros(\n",
    "            (total_size, context_size, num_layers, d_in),\n",
    "            dtype=self.config.dtype,\n",
    "            device=self.config.device,\n",
    "        )\n",
    "\n",
    "        for refill_batch_idx_start in refill_iterator:\n",
    "            refill_batch_tokens = self.get_batch_tokens()  ######\n",
    "            refill_activations = self.get_activations(refill_batch_tokens)\n",
    "\n",
    "            new_buffer[\n",
    "                refill_batch_idx_start : refill_batch_idx_start + batch_size, ...\n",
    "            ] = refill_activations\n",
    "\n",
    "            # pbar.update(1)\n",
    "\n",
    "        new_buffer = new_buffer.reshape(-1, num_layers, d_in)\n",
    "        new_buffer = new_buffer[torch.randperm(new_buffer.shape[0])]\n",
    "\n",
    "        return new_buffer\n",
    "\n",
    "    def get_data_loader(\n",
    "        self,\n",
    "    ) -> Iterator[Any]:\n",
    "        \"\"\"\n",
    "        Return a torch.utils.dataloader which you can get batches from.\n",
    "\n",
    "        Should automatically refill the buffer when it gets to n % full.\n",
    "        (better mixing if you refill and shuffle regularly).\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = self.config.train_batch_size_tokens\n",
    "\n",
    "        # 1. # create new buffer by mixing stored and new buffer\n",
    "        mixing_buffer = torch.cat(\n",
    "            [self.get_buffer(self.config.n_batches_in_buffer // 2), self.storage_buffer], ####\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        mixing_buffer = mixing_buffer[torch.randperm(mixing_buffer.shape[0])]\n",
    "\n",
    "        # 2.  put 50 % in storage\n",
    "        self.storage_buffer = mixing_buffer[: mixing_buffer.shape[0] // 2]\n",
    "\n",
    "        # 3. put other 50 % in a dataloader\n",
    "        dataloader = iter(\n",
    "            DataLoader(\n",
    "                # TODO: seems like a typing bug?\n",
    "                cast(Any, mixing_buffer[mixing_buffer.shape[0] // 2 :]),\n",
    "                batch_size=batch_size,\n",
    "                shuffle=True,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "    def next_batch(self):\n",
    "        \"\"\"\n",
    "        Get the next batch from the current DataLoader.\n",
    "        If the DataLoader is exhausted, refill the buffer and create a new DataLoader.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Try to get the next batch\n",
    "            return next(self.dataloader)\n",
    "        except StopIteration:\n",
    "            # If the DataLoader is exhausted, create a new one\n",
    "            self.dataloader = self.get_data_loader() #### 97\n",
    "            return next(self.dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 32768-L1-8e-05-LR-0.0004-Tokens-2.000e+06\n",
      "n_tokens_per_buffer (millions): 0.032896\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.000128\n",
      "Total training steps: 500000\n",
      "Total wandb updates: 5000\n",
      "n_tokens_per_feature_sampling_window (millions): 1.028\n",
      "n_tokens_per_dead_feature_window (millions): 5.14\n",
      "We will reset the sparsity calculation 500 times.\n",
      "Number tokens in sparsity calculation window: 4.00e+03\n",
      "Using Ghost Grads.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aw/projects/vitsearch/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: 32768-L1-8e-05-LR-0.0004-Tokens-3.447e+06\n",
      "n_tokens_per_buffer (millions): 0.032896\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.000128\n",
      "Total training steps: 861785\n",
      "Total wandb updates: 8617\n",
      "n_tokens_per_feature_sampling_window (millions): 1.028\n",
      "n_tokens_per_dead_feature_window (millions): 5.14\n",
      "We will reset the sparsity calculation 861 times.\n",
      "Number tokens in sparsity calculation window: 4.00e+03\n",
      "Using Ghost Grads.\n",
      "Run name: 32768-L1-8e-05-LR-0.0004-Tokens-3.447e+06\n",
      "n_tokens_per_buffer (millions): 0.032896\n",
      "Lower bound: n_contexts_per_buffer (millions): 0.000128\n",
      "Total training steps: 861785\n",
      "Total wandb updates: 8617\n",
      "n_tokens_per_feature_sampling_window (millions): 1.028\n",
      "n_tokens_per_dead_feature_window (millions): 5.14\n",
      "We will reset the sparsity calculation 861 times.\n",
      "Number tokens in sparsity calculation window: 4.00e+03\n",
      "Using Ghost Grads.\n",
      "{'n_layers': 24, 'd_model': 1024, 'd_head': 64, 'model_name': '', 'n_heads': 16, 'd_mlp': 4096, 'activation_name': 'gelu', 'eps': 1e-05, 'original_architecture': 'vit_clip_vision_encoder', 'initializer_range': 0.02, 'n_channels': 3, 'patch_size': 14, 'image_size': 224, 'n_classes': 768, 'n_params': None, 'layer_norm_pre': True, 'return_type': 'class_logits'}\n",
      "LayerNorm folded.\n",
      "Centered weights writing to residual stream\n"
     ]
    }
   ],
   "source": [
    "from sae_lens.training.sae_group import SparseAutoencoderDictionary\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "config = SAETrainConfig()\n",
    "processor = CLIPProcessor.from_pretrained(config.model_name)\n",
    "dataset = HFDataset(\"awilliamson/fashion-train\", processor.image_processor, \"image\", \"text\") # load_dataset(\"awilliamson/fashion-train\", split=\"train\")\n",
    "eval_dataset = HFDataset(\"awilliamson/fashion-eval\", processor.image_processor, \"image\", \"text\") # load_dataset(\"awilliamson/fashion-validation\", split=\"train\")\n",
    "# cfg.training_tokens = int(1_300_000*setup_args['num_epochs']) * cfg.context_size\n",
    "config.training_tokens = len(dataset) * config.context_size * config.num_epochs\n",
    "sae_group = SparseAutoencoderDictionary(config)\n",
    "model = HookedViT.from_pretrained(config.model_name, is_timm=False, is_clip=True)\n",
    "model.to(config.device)\n",
    "\n",
    "activation_store = OpenCLIPActivationsStore(\n",
    "    config = config,\n",
    "    model = model,\n",
    "    dataset = dataset,\n",
    "    eval_dataset = eval_dataset,\n",
    "    num_workers = 0,\n",
    ")\n",
    "\n",
    "for i, (name, sae) in enumerate(sae_group):\n",
    "    hyp = sae.cfg\n",
    "    print(\n",
    "        f\"{i}: Name: {name} Layer {hyp.hook_point_layer}, p_norm {hyp.lp_norm}, alpha {hyp.l1_coefficient}\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sae_group' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m AUTOENCODER_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlaion_CLIP-ViT-L-14-DataComp.XL-s13B-b90K_blocks.23.hook_mlp_out_16384_layers_23\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m sparse_autoencoder \u001b[38;5;241m=\u001b[39m \u001b[43msae_group\u001b[49m\u001b[38;5;241m.\u001b[39mautoencoders[AUTOENCODER_NAME]\n\u001b[1;32m      3\u001b[0m sparse_autoencoder \u001b[38;5;241m=\u001b[39m sparse_autoencoder\u001b[38;5;241m.\u001b[39mto(config\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m      4\u001b[0m layer_num \u001b[38;5;241m=\u001b[39m sparse_autoencoder\u001b[38;5;241m.\u001b[39mcfg\u001b[38;5;241m.\u001b[39mhook_point_layer\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sae_group' is not defined"
     ]
    }
   ],
   "source": [
    "AUTOENCODER_NAME = \"laion_CLIP-ViT-L-14-DataComp.XL-s13B-b90K_blocks.23.hook_mlp_out_16384_layers_23\"\n",
    "sparse_autoencoder = sae_group.autoencoders[AUTOENCODER_NAME]\n",
    "sparse_autoencoder = sparse_autoencoder.to(config.device)\n",
    "layer_num = sparse_autoencoder.cfg.hook_point_layer\n",
    "print(f\"Chosen layer {layer_num} hook point {sparse_autoencoder.cfg.hook_point}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sparse_autoencoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpress\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpx\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43msparse_autoencoder\u001b[49m\u001b[38;5;241m.\u001b[39meval()  \u001b[38;5;66;03m# prevents error if we're expecting a dead neuron mask for who grads\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      5\u001b[0m     batch_tokens, labels \u001b[38;5;241m=\u001b[39m activation_store\u001b[38;5;241m.\u001b[39mget_val_batch_tokens()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sparse_autoencoder' is not defined"
     ]
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "sparse_autoencoder.eval()  # prevents error if we're expecting a dead neuron mask for who grads\n",
    "with torch.no_grad():\n",
    "    batch_tokens, labels = activation_store.get_val_batch_tokens()\n",
    "    _, cache = model.run_with_cache(batch_tokens)\n",
    "    sae_out, feature_acts, loss, mse_loss, l1_loss, _ = sparse_autoencoder(\n",
    "        cache[sparse_autoencoder.cfg.hook_point]\n",
    "    )\n",
    "    # del cache\n",
    "\n",
    "    # ignore the bos token, get the number of features that activated in each token, averaged accross batch and position\n",
    "    l0 = (feature_acts[:, 1:] > 0).float().sum(-1).detach()\n",
    "    print(\"average l0\", l0.mean().item())\n",
    "    px.histogram(l0.flatten().cpu().numpy()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwillfulbytes\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/aw/projects/vitsearch/wandb/run-20240810_150314-h5d7rejr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/willfulbytes/openclip_sae_training/runs/h5d7rejr' target=\"_blank\">32768-L1-8e-05-LR-0.0004-Tokens-2.000e+06</a></strong> to <a href='https://wandb.ai/willfulbytes/openclip_sae_training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/willfulbytes/openclip_sae_training' target=\"_blank\">https://wandb.ai/willfulbytes/openclip_sae_training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/willfulbytes/openclip_sae_training/runs/h5d7rejr' target=\"_blank\">https://wandb.ai/willfulbytes/openclip_sae_training/runs/h5d7rejr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Objective value: 11732.4385:   4%|▍         | 4/100 [00:00<00:18,  5.14it/s]\n",
      "/Users/aw/projects/vitsearch/.venv/lib/python3.11/site-packages/sae_lens/training/sparse_autoencoder.py:279: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.tensor(origin, dtype=self.dtype, device=self.device)\n",
      "/Users/aw/projects/vitsearch/.venv/lib/python3.11/site-packages/sae_lens/training/train_sae_on_language_model.py:611: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=autocast)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interrupted, saving progress\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "torch.set_grad_enabled(True)\n",
    "from sae.train import train_sae_group_on_vision_model\n",
    "\n",
    "\n",
    "if config.log_to_wandb:\n",
    "    wandb.init(project=config.wandb_project, config=cast(Any, config), name=config.run_name)\n",
    "\n",
    "train_sae_group_on_vision_model(\n",
    "    model,\n",
    "    sae_group,\n",
    "    activation_store,\n",
    "    train_contexts=None, #TODO load checkpoints correctly to match saelens v2.1.3 lm_runner!\n",
    "    training_run_state=None,  #TODO load checkpoints correctly to match saelens v2.1.3 lm_runner!\n",
    "    n_checkpoints=config.n_checkpoints,\n",
    "    batch_size=config.train_batch_size_tokens,\n",
    "    feature_sampling_window=config.feature_sampling_window,\n",
    "    use_wandb=config.log_to_wandb,\n",
    "    wandb_log_frequency=config.wandb_log_frequency,\n",
    "    eval_every_n_wandb_logs=config.eval_every_n_wandb_logs,\n",
    "    autocast=config.autocast,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
